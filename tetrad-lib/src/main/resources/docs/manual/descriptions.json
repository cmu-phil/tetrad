{
  "pc": "PC algorithm (Spirtes and Glymour, Social Science Computer Review, 1991) is a CPDAG search which assumes that the underlying causal structure of the input data is acyclic, and that no two variables are caused by the same latent (unmeasured) variable. In addition, it is assumed that the input data set is either entirely continuous or entirely discrete; if the data set is continuous, it is assumed that the causal relation between any two variables is linear, and that the distribution of each variable is Normal. Finally, the sample should ideally be i.i.d.. Simulations show that PC and several of the other algorithms described here often succeed when these assumptions, needed to prove their correctness, do not strictly hold. The PC algorithm will sometimes output double-headed edges. In the large sample limit, double-headed edges in the output indicate that the adjacent variables have an unrecorded common cause, but PC tends to produce false positive double-headed edges on small samples.\nThe PC algorithm is correct whenever decision procedures for independence and conditional independence are available. The procedure conducts a sequence of independence and conditional independence tests, and efficiently builds a CPDAG from the results of those tests. As implemented in TETRAD, PC is intended for multinomial and approximately Normal distributions with i.i.d. data. The tests have an alpha value for rejecting the null hypothesis, which is always a hypothesis of independence or conditional independence. For continuous variables, PC uses tests of zero correlation or zero partial correlation for independence or conditional independence respectively. For discrete or categorical variables, PC uses either a chi square or a g square test of independence or conditional independence (see Causation, Prediction, and Search for details on tests). In either case, the tests require an alpha value for rejecting the null hypothesis, which can be adjusted by the user. The procedures make no adjustment for multiple testing. (For PC, CPC, FCI, all testing searches.)\nThe PC algorithm as given in Causation, Prediction and Search (Spirtes, Glymour, and Scheines, 2000) comes with three heuristics designed to reduce dependence on the order of the variables. The heuristic PC-1 simple sorts the variables in alphabetical order. The heuristic PC-2 and PC-3 sort edges by their p-values in the search. PP-3 further sorts parents of nodes in reverse order by the p-values of the conditional independence facts used to removed edges in the search. Please see Causation, Prediction, and Search for more details for these heuristics.\nNote that it is possible for PC with some choices of parameters to output bidirected edges or cycles, or to imply cycles by the Meek rules, against assumption. Bidirected edges can be prevented by choosing an appropriate collider conflict rule. Cycles can be prevented by setting the guaranteeCpdag parameter to 'true' ('Yes). This parameter has two effects. First, it prevents the orientation of any collider that would create a cycle in the graph. Second, whenever the final Meek rules attempt to directed an undirected edge as a directed edge, if this orientation would create a cycle, the edge is oriented in reverse, adding one or more new (arbitrary) unshielded triples to the graph. When this happens, the behavior is logged.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "cpc": "The CPC (Conservative PC) algorithm (Ramsey et al., ??) modifies the collider orientation step of PC to make it more conservative—that is, to increase the precision of collider orientations at the expense of recall. It does this as follows. Say you want to orient X—Y—Z as a collider or a noncollider; the PC algorithm looks at variables adjacent to X or variables adjacent to Z to find a subset S such that X is independent of Z conditional on S. The CPC algorithm considers all possible such sets and records the set on which X is conditionally independent of Z. If all of these sets contain Y, it orients X—Y—Z as a noncollider. If none of them contains Z, if orient X—Y—Z as a collider. If some contain Z but other don’t, it marks it as ambiguous, with an underline. Thus, the output is ambiguous between CPDAGs; in order to get a specific CPDAG out of the output, one needs first to decide whether the underlined triples are colliders or noncolliders and then to apply the orientation rules in Meek (1997).\nThe PC algorithm is correct whenever decision procedures for independence and conditional independence are available. The procedure conducts a sequence of independence and conditional independence tests, and efficiently builds a CPDAG from the results of those tests. As implemented in TETRAD, PC is intended for multinomial and approximately Normal distributions with i.i.d. data. The tests have an alpha value for rejecting the null hypothesis, which is always a hypothesis of independence or conditional independence. For continuous variables, PC uses tests of zero correlation or zero partial correlation for independence or conditional independence respectively. For discrete or categorical variables, PC uses either a chi square or a g square test of independence or conditional independence (see Causation, Prediction, and Search for details on tests). In either case, the tests require an alpha value for rejecting the null hypothesis, which can be adjusted by the user. The procedures make no adjustment for multiple testing. (For PC, CPC, FCI, all testing searches.)\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "pc-max": "Similar in spirit to CPC but orients all unshielded triples using maximum likelihood conditioning sets. The idea is as follows. The adjacency search is the same as for PC, but colliders are oriented differently. Let X—Y—Z be an unshielded triple (X not adjacent to Z) and find all subsets S from among the adjacents of X or the adjacents of Z such that X is independent of Z conditional on S. However, instead of using the CPC rule to orient the triple, instead just list the p-values for each of these conditional independence judgments and pick the set S’ that yields the highest such p-value. Then orient X->Y<-Z if S does not contain Y and X—Y—Z otherwise. This orients all unshielded triples. It’s possible (though rare) that adjacent triples both be oriented as 2-cycles, X->Y<->Z<-W. If this happens, pick one of the other of these triples or orient as a collider, arbitrarily. This guarantees that the resulting graph will be a CPDAG.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "fges": "FGES is an optimized and parallelized version of an algorithm developed by Meek [Meek, 1997] called the Greedy Equivalence Search (GES). The algorithm was further developed and studied by Chickering [Chickering, 2002]. GES is a Bayesian algorithm that heuristically searches the space of CBNs and returns the model with the highest Bayesian score it finds. In particular, GES starts its search with the empty graph. It then performs a forward stepping search in which edges are added between nodes in order to increase the Bayesian score. This process continues until no single edge addition increases the score. Finally, it performs a backward stepping search that removes edges until no single edge removal can increase the score. More information is available here and here. The reference is Ramsey et al., 2017.\nThe algorithm requires a decomposable score—that is, a score that for the entire DAG model is a sum of logged scores of each variables given its parents in the model. The algorithms can take all continuous data (using the SEM BIC score), all discrete data (using the BDeu score) or a mixture of continuous and discrete data (using the Conditional Gaussian score); these are all decomposable scores. Note that in all case, BIC is calculated as 2L - k ln N, so \"higher is better.\"\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.\nNote: It is possible to run FGES followed some non-Gaussian orientation algorithm like FASK-pairwise or R3 or RSkew. To do this, see the FASK algorithm. There one can select an algorithm to estimate adjacencies and a pairwise algorithm to estimate orientations. This is for the linear, non-Gaussian case, where such pairwise algorithms are effective.",
  "images": "Adjusts the selected score for FGES to allow for multiple datasets as input. The linear, Gaussian BIC scores for each data set are averaged at each step of the algorithm, producing a model for all data sets that assumes they have the same graphical structure across dataset. Note that BIC is calculated as 2L - k ln N, so \"higher is better.\"",
  "images-boss": "Wraps the IMaGES algorithm for continuous variables. This version uses the BOSS algorithm in place of FGES.\nRequires that the parameter 'randomSelectionSize' be set to indicate how many datasets should be taken at a time (randomly). This cannot be given multiple values.",
  "fci": "The FCI algorithm is a constraint-based algorithm that takes as input sample data and optional background knowledge and in the large sample limit outputs an equivalence class of CBNs that (including those with hidden confounders) that entail the set of conditional independence relations judged to hold in the population. It is limited to several thousand variables, and on realistic sample sizes it is inaccurate in both adjacencies and orientations. FCI has two phases: an adjacency phase and an orientation phase. The adjacency phase of the algorithm starts with a complete undirected graph and then performs a sequence of conditional independence tests that lead to the removal of an edge between any two adjacent variables that are judged to be independent, conditional on some subset of the observed variables; any conditioning set that leads to the removal of an adjacency is stored. After the adjacency phase, the resulting undirected graph has the correct set of adjacencies, but all the edges are unoriented. FCI then enters an orientation phase that uses the stored conditioning sets that led to the removal of adjacencies to orient as many of the edges as possible. See [Spirtes, 1993].\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "fci-max": "The FCI-Max algorithm simply changes the first collider orientation rule in FCI to use the PC-Max orientation.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "cfci": "Adjusts FCI (see) to use conservative orientation as in CPC (see). Because the collider orientation is conservative, there may be ambiguous triples; these may be retrieved using that accessor method.\nThis class is configured to respect knowledge of forbidden and required edges, including knowledge of temporal tiers.",
  "rfci": "A modification of the FCI algorithm in which some expensive steps are finessed and the output is somewhat differently interpreted. In most cases this runs faster than FCI (which can be slow in some steps) and is almost as informative. See Colombo et al., 2012.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "pag-sampling-rfci": "A modification of the RFCI algorithm which does probabilistic bootstrap sampling with respect to the RFCI PAG output.For discrete data only. Parameters are: (a) Number of search probabilistic models, (b) boostrap ensemble method to use (see bootstrapping), (c) maximum size of conditioning set (depth), (d) maximum length of any discriminating path (a property for RFCI). This must use the probabilistic test, which must be selected. Parameters for the probabilistic test are (d) independence cutoff threshold, default 0.5, (e) prior equivalent sample size, and (f) whether the cutoff in (d) is used in the independence test calculation; if not, then a coin flip is used (probability 0.5).",
  "gfci": "GFCI is a combination of the FGES [FGES, 2016] algorithm and the FCI algorithm [Spirtes, 1993] that improves upon the accuracy and efficiency of FCI. In order to understand the basic methodology of FCI, it is necessary to understand some basic facts about the FGES and FCI algorithms. The FGES algorithm is used to improve the accuracy of both the adjacency phase and the orientation phase of FCI by providing a more accurate initial graph that contains a subset of both the non-adjacencies and orientations of the final output of FCI. The initial set of nonadjacencies given by FGES is augmented by FCI performing a set of conditional independence tests that lead to the removal of some further adjacencies whenever a conditioning set is found that makes two adjacent variables independent. After the adjacency phase of FCI, some of the orientations of FGES are then used to provide an initial orientation of the undirected graph that is then augmented by the orientation phase of FCI to provide additional orientations. Note: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "LV-Heuristic": "LV-Heuristic is a heuristic latent variable algorithm the calculates a DAG (directed acyclic graph) using the BOSS algorithm (see) and the simply reports the PAG (partial ancestral graph) that this DAG belongs to. It is a quite accurate algorithm, but it is not correct. LV-Heuristic serves as the initial PAG estimate for the FCIT (FCI-Targeted-testing) algorithm (see), which does the extra work to turn the LV-Heuristic PAG estimate into a correct PAG. Note that the LV-Heuristic algorithm, since it does not do extra work to turn the graph into a correct PAG, cannot orient bidirected edges.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "FCIT": "FCIT with the LV-Heuristic (see) output (i.e., runs BOSS, estimates a DAG, then reports the PAG that this DAG belongs to), then does extra work to turn this PAG into a correct PAG for the latent variable case. This work consists in using a recursive path blocking algorithm to find certain sepsets consistent with the full orientations of interim PAGs. The recursive blocking algorithm is not completely effective at finding all necessary sepsets, so the algorithm is optionally finished with the adjacency sepset finding method from FCI, which renders the algorithm correct under the Markov and Faithfulness assumption (i.e., using an Oracle method to calculate m-separation facts). This final use of the adjacency sepset method, while allowing the algorithm to be correct in Oracle mode, can reduce accuracy of the algorithm, which is optimized using the recursive blocking edge removal step, so a parameter is included to allower the to turn the final adjacency sepset section of the algorithm off.\nAfter each edge removal after the initial LV-Heuristic PAG estimate, the edge-reduced PAG is re-oriented so that it is a legal PAG. The implication is that the final graph returned by the algorithm is always a legal PAG, so a final legal PAG pipeline need not be applied and is not made available for this algorithm.\n\nFCIT starts with BOSS or GRaSP and gets a valid order and a DAG/CPDAG. It sets the initial state of the estimted PAG to this CPDAG and orients as o-o and then copy all of the unshielded colliders from the CPDAG to the PAG. It then performs a testing modified from the step in FGES-FCI that is more efficient. In GFCI, an edge x *-* y is removed from the graph is there is a subset S of adj(x) \\ {y} or adj(y) \\ {x} such that x _||_ y | S. For FCIT we instead search outward from x for a set S using a novel procedure that blocks all existing paths in the estimated PAG either unconditionally or conditionally. Bounds may be placed on the lengths of the blocked paths to consider and the depth (maximum |S|). We also allow bounds on the time spent on any testing step. As edges are removed in this way, any additional colliders are oriented given these sepsets S. We then run the final FCI orientation rules, where for the discriminating path rule we use the same \"out of x\" sepset finding idea. We optionally allow the user to request that a legal PAG be returned using a novel procedure; this guarantee has been extended to all latent variable algorithms in Tetrad that return partial ancestral graphs (PAGs).\nFCIT, along with BOSS-FCI, can produce highly accurate models in simulation. FCIT, in particular, is highly scalable. A paper describing BOSS-FCI and FCIT in more detail is planned. We make both BOSS-FCI and FCIT non-experimental for this version since requests have been made to use them.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "grasp-fci": "GRaSP-FCI is wrapper around the GRaSP algorithm that replaces the FGES step in FGES-FCI with the more accurate GRaSP algorithm, which reasons by considering permutations of variables. The second collider orientation step in FGES-FCI is also done using permutation reasoning, leaving the discriminating path rule as the only rule that requires a \"raw\" conditional independence judgment. Ultimately this independence judgment can be decided using the same scoring apparatus as the other permutation steps, so ultimately GRaSP-FCI can be treated as a scoring algorithm.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "boss-fci": "Uses BOSS in place of FGES for the initial step in the GFCI algorithm. This tends to produce a accurate PAG than GFCI as a result, for the latent variables case. This is a simple substitution; the reference for GFCI is here:\nJ.M. Ogarrio and P. Spirtes and J. Ramsey, \"A Hybrid Causal Search Algorithm for Latent Variable Models,\" JMLR 2016. Here, BOSS has been substituted for FGES.\nFor BOSS only a score is needed, but there are steps in GFCI that require a test, so for this method, both a test and a score need to be given. The reference for BOSS is:\nAndrews, B., Ramsey, J., Sanchez Romero, R., Camchong, J., & Kummerfeld, E. (2023). Fast scalable and accurate discovery of dags using the best order score search and grow shrink trees. Advances in Neural Information Processing Systems, 36, 63945-63956.\nThis class is configured to respect knowledge of forbidden and required edges, including knowledge of temporal tiers.\nFCIT, along with BOSS-FCI, can produce highly accurate models in simulation. FCIT, in particular, is highly scalable. A paper describing BOSS-FCI and FCIT in more detail is planned. We make both BFCI and FCIT non-experimental for this version since requests have been made to use them.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "spfci": "SP-FCI is wrapper around the SP algorithm that replaces the FGES step in GFCI with the more accurate SP algorithm, which reasons by considering permutations of variables. This uses the same method for wrapping SP with an FCI method similar to GFCI as GRaSP-FCI. The difference is that SP considers every permutation of the variables and so necessarily return a frugal DAG. This can only be used for very small models, of up to about 8 or 9 variables, because of the super-exponential step of considering every permutation of the variables. The second collider orientation step in GFCI is done using permutation reasoning, leaving the discriminating path rule as the only rule that requires a \"raw\" conditional independence judgment. Ultimately this independence judgment can be decided using the same scoring apparatus as the other permutation steps, so ultimately GRaSP-FCI can be treated as a scoring algorithm.",
  "fci-iod": "Runs FCI on multiple datasets using the IOD pooled dataset independence test. The reference is here:\nTillman, R., & Spirtes, P. (2011, June). Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (pp. 3-15). JMLR Workshop and Conference Proceedings.",
  "svar-fci": "The SvarFCI algorithm is a version of FCI for time series data. See the FCI documentation for a description of the FCI algorithm, which allows for unmeasured (hidden, latent) variables in the data-generating process and produces a PAG (partial ancestral graph). svarFCI takes as input a “time lag data set,” i.e., a data set which includes time series observations of variables X1, X2, X3, ..., and their lags X1:1, X2:1, X3:1, ..., X1:2, X2:2,X3:2, ... and so on. X1:n is the nth-lag of the variable X1. To create a time lag data set from a standard tabular data set (i.e., a matrix of observations of X1, X2, X3, ...), use the “create time lag data” function in the data manipulation toolbox. The user will be prompted to specify the number of lags (n), and a new data set will be created with the above naming convention. The new sample size will be the old sample size minus n.\nSince this algorithm specifically requires time series data, one must set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform.",
  "svar-gfci": "SvarGFCI uses a BIC score to search for a skeleton. Thus, the only user-specified parameter is an optional “penalty score” to bias the search in favor of more sparse models. See the description of the GES algorithm for discussion of the penalty score. For the traditional definition of the BIC score, set the penalty to 1.0. The orientation rules are the same as for FCI. As is the case with SvarFCI, SvarFCI will automatically respect the time order of the variables and impose a repeating structure. Firstly, it puts lagged variables in appropriate tiers so, e.g., X3:2 can cause X3:1 and X3 but X3:1 cannot cause X3:2 and X3 cannot cause either X3:1 or X3:2. Also, it will assume that the causal structure is the same across time, so that if the edge between X1 and X2 is removed because this increases the BIC score, then also the edge between X1:1 and X2:1 is removed, and so on for additional lags if they exist. When some edge is removed as the result of a score increase, all similar (or “homologous”) edges are also removed. Note that BIC is calculated as 2L - k ln N, so \"higher is better.\"\nSince this algorithm specifically requires time series data, one must set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform.",
  "ccd": "CCD assumes that the data are causally sufficient (no latent variables) though possibly with directed cycles. No background knowledge is permitted. It generates a \"cyclic PAG\". See Richardson, T. S. (2013). A discovery algorithm for directed cyclic graphs. arXiv preprint arXiv:1302.3599, for details. Note that the output graph contains circle endpoints as with a latent variable PAG, but these are interpreted differently. CCD reasons about cyclic (feedback) models using conditional independence facts alone, as with PC or FCI, so is general in this sense.",
  "fges-mb": "This is a restriction of the FGES algorithm to union of edges over the combined Markov blankets of a set of targets, including the targets. In the interface, just one target may be specified. See Ramsey et al., 2017 for details. In the general case, finding the graph over the Markov blanket variables of a target (including the target) is far faster than finding the CPDAG for all the variables.",
  "pc-mb": "PC-MB. Similar to FGES-MB (see FGES, 2016) but using PC as the basic search instead of FGES. The rules of the PC search are restricted to just the variables in the Markov blanket of a target T, including T; the result is a graph that is a CPDAG over these variables.",
  "fas": "This is just the adjacency search of the PC algorithm, included here for times when just the adjacency search is needed, as when one is subsequently just going to orient variables pairwise.",
  "sp": "SP (Sparsest Permutation, Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183) searches for model satisfying the SMR (frugality) assumption for small models of up to about 9 variables. The algorithms works searching over all possible permutations of the variables and building DAGs for them in the same way as the GRaSP algorithm. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made.\nThe algorithms works searching over all possible permutations of the variables and building DAGs for them in the same way as the GRaSP algorithm. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made.",
  "grasp": "GRaSP (Greedy Relations of Sparsest Permutation) is an algorithm that generalizes and extends the GSP (Greedy Sparsest Permutation) algorithm. Is generalizes specifically the algorithms TSP (or which GSP is a bounded and iterated version) and ESP (Solus et al., 2017) by allowing those algorithms to equivalently be run by selecting particular parameterizations of GRaSP, where ESP enlarges the search space of GSP. The implementation given for ESP renders that algorithm tractable. By choosing other parameterizations of GRaSP, it is possible to further enlarge the search space even over and above that of ESP to render the algorithm significantly more accurate. In all cases, these algorithms come back quickly enough to analyze accurately sparse problems of up to 300 variables and denser problems with up to about 100 variables. The parameters are as follows: graspDepth - This controls the overall recursion depth for a depth first search. graspNonsingularDepth - This controls the depth at which nonsingular tucks are explored. graspSingularDepth - This controls the depth at which singular tucks are considered. numRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nThe parameters are as follows: graspDepth - This controls the overall recursion depth for a depth first search. graspNonsingularDepth - This controls the depth at which nonsingular tucks are explored. graspSingularDepth - This controls the depth at which singular tucks are considered. numRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\ngraspDepth - This controls the overall recursion depth for a depth first search. graspNonsingularDepth - This controls the depth at which nonsingular tucks are explored. graspSingularDepth - This controls the depth at which singular tucks are considered. numRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\ngraspNonsingularDepth - This controls the depth at which nonsingular tucks are explored. graspSingularDepth - This controls the depth at which singular tucks are considered. numRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\ngraspSingularDepth - This controls the depth at which singular tucks are considered. numRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nnumRestarts - By default 1; if > 1, additional random restarts are done, and the best of these results is returned. TSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nTSP corresponds to singular depth 0, nonsingular depth 0. ESP corresponds to singular depth > 0, nonsingular depth = 0. GRaSP corresponds to singular depth > 0, nonsingular depth > 0. In each case, an ordering option is available to find the best permutations from lower levels before proceeding to higher levels. The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nThe algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. Two ways of building DAGs are considered, one independence-based, due to Raskutti and Uhler and a score-based method, using the Grow-Shrink (GS) method (Margaritis and Thrun, 1999). If the Pearl method is selected, an independence test will be used; if the GS method is selected, a score will be used, so both a test and a score need to be supplied so that this choice can be made. We recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nWe recommend that the user turn on logging to watch the progress of the algorithm; this helps with larger searches especially. Knowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nKnowledge of forbidden edges may be used with GRaSP; currently knowledge of required edges is not implemented. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nDimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nRaskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nSolus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530.\nLam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.",
  "boss": "BOSS (Best Order Score Search) is an algorithm that, like GRaSP, generalizes and extends the GSP (Greedy Sparsest Permutation) algorithm. It has been tested to 1000 variables with an average degree of 20 and gives near perfect precisions and recalls for N = 10,000 (with recall that drop to 0.9 for N = 1000). The algorithm works by building DAGs given permutations in ways similar to those described in Raskutti and Uhler (ref?) and Solus et al. The parameters are as follows: useBes - True if the final BES (Backward Equivalence Search) step is used from the GES (Greedy Equivalence Search) algorithm. This step is needed for correctness but for large models, since usually nearly all edges are oriented in the CPDAG, it is heuristically not needed. numStarts - The number of times the algorithm should be re-run from different random starting permutations. The model with the most optimal BIC score will be selected. allowInternalRandomness - If true, the algorithm allow the algorithm to use certain heuristic random steps. This can improve performance, but may make the algorithm non-deterministic. timeLag - This creates a time-series model automatically with a certain number of lags. Knowledge of forbidden edges and required edges may be used with this algorithm. Also, knowledge of tiers may be used. If tiered knowledge is supplied, the algorithm will analyze the tiers in order, so that the time required for the algorithm is linear in the number of tiers. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nThe parameters are as follows: useBes - True if the final BES (Backward Equivalence Search) step is used from the GES (Greedy Equivalence Search) algorithm. This step is needed for correctness but for large models, since usually nearly all edges are oriented in the CPDAG, it is heuristically not needed. numStarts - The number of times the algorithm should be re-run from different random starting permutations. The model with the most optimal BIC score will be selected. allowInternalRandomness - If true, the algorithm allow the algorithm to use certain heuristic random steps. This can improve performance, but may make the algorithm non-deterministic. timeLag - This creates a time-series model automatically with a certain number of lags. Knowledge of forbidden edges and required edges may be used with this algorithm. Also, knowledge of tiers may be used. If tiered knowledge is supplied, the algorithm will analyze the tiers in order, so that the time required for the algorithm is linear in the number of tiers. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nuseBes - True if the final BES (Backward Equivalence Search) step is used from the GES (Greedy Equivalence Search) algorithm. This step is needed for correctness but for large models, since usually nearly all edges are oriented in the CPDAG, it is heuristically not needed.\nnumStarts - The number of times the algorithm should be re-run from different random starting permutations. The model with the most optimal BIC score will be selected.\nallowInternalRandomness - If true, the algorithm allow the algorithm to use certain heuristic random steps. This can improve performance, but may make the algorithm non-deterministic.\ntimeLag - This creates a time-series model automatically with a certain number of lags. Knowledge of forbidden edges and required edges may be used with this algorithm. Also, knowledge of tiers may be used. If tiered knowledge is supplied, the algorithm will analyze the tiers in order, so that the time required for the algorithm is linear in the number of tiers. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nKnowledge of forbidden edges and required edges may be used with this algorithm. Also, knowledge of tiers may be used. If tiered knowledge is supplied, the algorithm will analyze the tiers in order, so that the time required for the algorithm is linear in the number of tiers. Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nDimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999. Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nRaskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183. Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530. Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.\nSolus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530.\nLam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.",
  "bpc": "Searches for causal structure over latent variables, where the true models are Multiple Indicator Models (MIM’s) as described in the Graphs section. The idea is this. There is a set of latent (unmeasured) variables over which a directed acyclic model has been defined. Then for each of these latent L there are 3 (preferably 4) or more measures of that variable—that is, measured variables that are all children of L. Under these conditions, one may define tetrad constraints (see Spirtes et al., 2000). There is a theorem to the effect that if certain CPDAGs of these tetrad constraints hold, there must be a latent common cause of all of them (the Tetrad Representation Theorem, see Silva et al., 2003, where the BPC (“Build Pure Clusters”) algorithm is defined and discussed.) Moreover, once one has such a “measurement model,” once can estimate a covariance matrix over the latent variables that are parents of the measures and use some algorithm such as PC or GES to estimate a CPDAG over the latents. The algorithm to run PC or GES on this covariance matrix is called MimBuild (“MIM” is the graph, Multiple Indicator Model; “Build” means build). In this way, one may recover causal structure over the latents. The more measures one has for each latent, the better the result is, generally. The larger the sample size the better. One important issue is that the algorithm is sensitive to so-called “impurities”—that is, causal edges among the measured variables, or between measured variables and unintended latent. The algorithm will in effect remove one measure in each impure pair from consideration. The algorithm to run PC or GES on this covariance matrix is called MimBuild (“MIM” is the graph, Multiple Indicator Model; “Build” means build). MimBUILD is an optional choice inside FOFC In this way, one may recover causal structure over the latents. The more measures one has for each latent the better the result is, generally. A reference for the Wishart and Delta Tetrad tests is: Bollen, K. A., & Ting, K. F. (1993). Confirmatory tetrad analysis. Sociological methodology, 147-175. The original reference for the Delta Tetrad test is Bollen, K. A. (1990). Outlier screening and a distribution-free test for vanishing tetrads. Sociological Methods & Research, 19(1), 80-92.",
  "fofc": "Searches for causal structure over latent variables, where the true models are Multiple Indicator Models (MIM’s) as described in the Graphs section. The idea is this. There is a set of latent (unmeasured) variables over which a directed acyclic model has been defined, Then for each of these latent L there are 3 (preferably 4) or more measures of that variable—that is, measured variables that are all children of L. Under these conditions, one may define tetrad constraints (see Spirtes et al., 2000). There is a theorem to the effect that if certain CPDAGs of these tetrad constraints hold, there must be a latent common cause of all of them (the Tetrad Representation Theorem). The FOFC (Find One Factor Clusters) takes advantage of this fact. The basic idea is to build up clusters one at a time by adding variables that keep them pure in the sense that all relevant tetrad constraints still hold. There are different ways of going about this. One could try to build one cluster up as far as possible, then remove all of those variables from the set, and try to make another cluster using the remaining variables (SAG, Seed and Grow). Or one can try in parallel to grow all possible clusters and then choose among the grown clusters using some criterion such as cluster size (GAP, Grow and Pick). In general, GAP is more accurate. The result is a clustering of variables. Once one has such a “measurement model, one can estimate (using the ESTIMATOR box) a covariance matrix over the latent variables that are parents of the measures and use some algorithm such as PC or GES to estimate a CPDAG over the latent variables. The algorithm to run PC or GES on this covariance matrix is called MimBuild (“MIM” is the graph, Multiple Indicator Model; “Build” means build). MimBUILD is an optional choice inside FOFC In this way, one may recover causal structure over the latents. The more measures one has for each latent the better the result is, generally. At least 3 measured indicator variables are needed for each latent variable. The larger the sample size the better. One important issue is that the algorithm is sensitive to so-called “impurities”—that is,causal edges among the measured variables, or between measured variables and multiple latent variables. The algorithm will in effect remove one measure in each impure pair from consideration. Note that for FOFC, a test is done for each final cluster to see whether the variables in the cluster are all mutually dependent. In the interface, in order to see teh results of this test, one needs to open the logging window. See the Logging menu.",
  "ftfc": "FTFC (Find Two Factor Clusters) is similar to FOFC, but instead of each cluster having one latent that is the parent of all the measure in the cluster, it instead has two such latents. So each measure has two latent parents; these are two “factors.” Similarly to FOFC, constraints are checked for, but in this case, the constraints must be sextad constraints, and more of them must be satisfied for each pure cluster (see Kummerfeld et al., 2014). Thus, the number of measures in each cluster, once impure edges have been taken into account, must be at least six, preferably more.",
  "ica-lingam": "ICA LiNGAM (Shimizu et al., 2006) was one of the first of the algorithms that assumed linearity among the variables and non-Gaussianity of error term, and still one of the best for smaller models, for the basic algorithm, implemented here. The idea is to use the Independent Components Analysis (ICA) algorithm to check all permutations of the variables to find one that is a causal order—that is, one in which earlier variables can cause later variables but not vice-versa. The method is clever. First, since we assume the model is a directed acyclic graph (DAG), there must be some permutation of the variables for which the main diagonal of the inverse of the weight matrix contains no zeros. This gives us a permuted estimate of the weight matrix. Then we look for a permutation of this weight matrix that is lower triangular. There must be one, since the model is assumed to be a DAG. But a lower triangular weight matrix just gives a causal order, so we’re done.\nIn the referenced paper, we implement Algorithm A, which is described above. Once one has a causal order the only thing one needs to do is to eliminate the extra edges. For this, we use the causal order to define knowledge of tiers and run FGES.\nOur implementation of LiNGAM has one parameter, penalty discount, used for the FGES adjacency search. The method as implemented does not scale much beyond 10 variables, because it is checking every permutation of all the variables (twice). The implementation of ICA we use is FastIca (Hyvärinen et al., 2004).\nShimizu, S., Hoyer, P. O., Hyvärinen, A., & Kerminen, A. (2006). A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct), 2003-2030.\nHyvärinen, A., Karhunen, J., & Oja, E. (2004). Independent component analysis (Vol. 46). John Wiley & Sons.",
  "ica-ling-d": "Implements the ICA-LiNG-D algorithm as well as a number of ancillary methods for LiNG-D and LiNGAM. The reference is here:\nLacerda, G., Spirtes, P. L., Ramsey, J., & Hoyer, P. O. (2012). Discovering cyclic causal models by independent components analysis. arXiv preprint arXiv:1206.3273.\nICA-LING-D is a method for estimating a possible cyclic linear models graph from a dataset. It is based on the assumption that the data are generated by a linear model with non-Gaussian noise. The method is based on the following assumptions:\nlower bound of the absolute value of the coefficients in the W matrix.\nUnder these assumptions, the method estimates a matrix W such that WX = e, where X is the data matrix, e is a matrix of noise, and W is a matrix of coefficients. The matrix W is then used to estimate a matrix B Hat, where B Hat is the matrix of coefficients in the linear model that generated the data. The graph is then estimated by finding edges in B Hat.\nWe use the N Rooks algorithm to find alternative diagonals for permutations of the W matrix. The parameter that N Rooks requires is a threshold for entries in W to be included in possible diagonals, which is the lowest number in absolute value that a W matrix entry can take to be part of a diagonal; the implied permutations is the permutations that permutes rows so that these combination lies along the their respective diagonals in W, which are then scaled, and the separate satisfactory B Hat matrices reported.\nICA-LiNG-D, which takes this W as an impute, is a method for estimating a directed graph (DG) from a dataset. The graph is estimated by finding a permutation of the columns of the dataset so that the resulting matrix has a strong diagonal. This permutation is then used to estimate a DG. The method is an relaxation of LiNGAM, which estimates a DAG from a dataset using independent components analysis (ICA). LiNG-D is particularly useful when the underlying data may have multiple consistent cyclic models.\nThis class is not configured to respect knowledge of forbidden and required edges.",
  "fask": "FASK learns a linear model in which all the variables are skewed.\nThe idea is as follows. First, FAS-stable is run on the data, producing an undirected graph. We use the BIC score as a conditional independence test with a specified penalty discount c. This yields undirected graph G0 . The reason FAS-stable works for sparse cyclic models where the linear coefficients are all less than 1 is that correlations induced by long cyclic paths are statistically judged as zero, since they are products of multiple coefficients less than 1. Then, each of the X − Y adjacencies in G0 is oriented as a 2-cycle X += Y , or X → Y , or X ← Y . Taking up each adjacency in turn, one tests to see whether the adjacency is a 2-cycle by testing if the difference between corr(X, Y ) and corr(X, Y |X > 0), and corr(X, Y ) and corr(X, Y |Y > 0), are both significantly not zero. If so, the edges X → Y and X ← Y are added to the output graph G1 . If not, the Left-Right orientation is rule is applied: Orient X → Y in G1, if (E(X Y |X > 0)/ E(X 2|X > 0)E(Y 2 |X > 0) − E(X Y |Y > 0)/ E(X 2 |Y > 0)E(Y 2|Y > 0)) > 0; otherwise orient X ← Y . G1 will be a fully oriented graph. For some models, where the true coefficients of a 2-cycle between X and Y are more or less equal in magnitude but opposite in sign, FAS-stable may fail to detect an edge between X and Y when in fact a 2-cycle exists. In this case, we check explicitly whether corr(X, Y |X > 0) and corr(X, Y |Y > 0) differ by more than a set amount of 0.3. If so, the adjacency is added to the graph and oriented using the aforementioned rules.\nWe include pairwise orientation rule RSkew, Skew, and Tanh from Hyvärinen, A., & Smith, S. M. (2013). Pairwise likelihood ratios for estimation of non-Gaussian structural equation models. Journal of Machine Learning Research, 14(Jan), 111-152, so in some configurations FASK can be made to implement an algorithm that has been called in the literature \"Pairwise LiNGAM\"--this is intentional; we do this for ease of comparison. You'll get this configuration if you choose one of these pairwise orientation rules, together with the FAS with orientation alpha and two-cycle threshold set to zero and skewness threshold set to 1, for instance.\nSee Sanchez-Romero R, Ramsey JD, Zhang K, Glymour MR, Huang B, Glymour C. Causal discovery of feedback networks with functional magnetic resonance imaging. Network Neuroscience 2018.",
  "direct-lingam": "Implements the Direct-LiNGAM algorithm. The reference is here:\nS. Shimizu, T. Inazumi, Y. Sogawa, A. Hyvärinen, Y. Kawahara, T. Washio, P. O. Hoyer and K. Bollen. DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model. Journal of Machine Learning Research, 12(Apr): 1225–1248, 2011.\nA. Hyvärinen and S. M. Smith. Pairwise likelihood ratios for estimation of non-Gaussian structural evaluation models. Journal of Machine Learning Research 14:111-152, 2013.",
  "dagma": "Implements the DAGMA algorithm. The reference is here:\nBello, K., Aragam, B., & Ravikumar, P. (2022). Dagma: Learning dags via m-matrices and a log-determinant acyclicity characterization. Advances in Neural Information Processing Systems, 35, 8226-8239.",
  "fask-vote": "FASK-Vote is a metascript that learns a model from a list of datasets in a method similar to IMaGES (see). For adjacencies, it uses FAS-Stable with the voting-based score from IMaGES used as a test (using all the datasets, standardized), producing a single undirected graph G. It then orients each edge X--Y in G for each dataset using the FASK (see) left-right rule and orient X->Y if that rule orients X--Y as such in at least half of the datasets. The final graph is returned.\nFor FASK, See Sanchez-Romero R, Ramsey JD, Zhang K, Glymour MR, Huang B, Glymour C. Causal discovery of feedback networks with functional magnetic resonance imaging. Network Neuroscience 2018.\n\nFASK-Vote is a metascript that learns a model from a list of datasets in a method similar to IMaGES (see). For adjacencies, it uses FAS-Stable with the voting-based score from IMaGES used as a test (using all the datasets, standardized), producing a single undirected graph G. It then orients each edge X--Y in G for each dataset using the FASK (see) left-right rule and orient X->Y if that rule orients X--Y as such in at least half of the datasets. The final graph is returned.\nFor FASK, See Sanchez-Romero R, Ramsey JD, Zhang K, Glymour MR, Huang B, Glymour C. Causal discovery of feedback networks with functional magnetic resonance imaging. Network Neuroscience 2018.",
  "r3": "This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses an entropy calculation to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, the orientation is fairly easy. This rule is similar to Hyvarinen and Smith's (2013) EB rule, but using Anderson Darling for the measure of non-Gaussianity, to somewhat better effect. See Ramsey et al. (2012).\n\nThis is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses an entropy calculation to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, the orientation is fairly easy. This rule is similar to Hyvarinen and Smith's (2013) EB rule, but using Anderson Darling for the measure of non-Gaussianity, to somewhat better effect. See Ramsey et al. (2012).",
  "skew": "This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses a skewness to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.\nThe Skew rule is differently motivated from the RSkew rule (see), though they both appeal to the skewness of the variables.\n\nThis is an algorithm that orients an edge X--Y for continuousz variables based on non-Gaussian information. This rule in particular uses a skewness to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.\nThe Skew rule is differently motivated from the RSkew rule (see), though they both appeal to the skewness of the variables.",
  "r-skew": "This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses a skewness to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.\nThe RSkew rule is differently motivated from the Skew rule (see), though they both appeal to the skewness of the variables.",
  "fask-pw": "This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses the FASK pairwise rule to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.\nThe FASK-PW rule appeals to skewness in a different way than Skew and RSkew.",
  "dm-pc": "Implements the DM-PC (Detect MIMIC PC) algorithm, which searches for intermediate latents using principles similar to PC.",
  "dm-fcit": "Implements the DM-FCIT (Detect MIMIC FCIT) algorithm, which searches for intermediate latents using the FCIT algorithm (which uses both scoring and testing) in place of the PC-style principles of DM-PC.",
  "boss-lingam": "Implements the BOSS-LiNGAM algorithm which first finds a CPDAG for the variables and then uses a non-Gaussian orientation method to orient the undirected edges. The reference is as follows:\nHoyer et al., \"Causal discovery of linear acyclic models with arbitrary distributions,\" UAI 2008.\nThe test for normality used for residuals is Anderson-Darling, following 'ad.test' in the nortest package of R. The default alpha level is 0.05--that is, p values from AD below 0.05 are taken to indicate nongaussianity.\nIt is assumed that the CPDAG is the result of a CPDAG search such as PC or GES. In any case, it is important that the residuals be independent for ICA to work.\nThis may be replaced by a more general algorithm that allows alternatives for the CPDAG search and for the the non-Gaussian orientation method.\nThis class is not configured to respect knowledge of forbidden and required edges.\nNote: If one wants to analyze time series data using this algorithm, one may set the time lag parameter to a value greater than 0, which will automatically apply the time lag transform. The same goes for any algorithm that has this parameter available in the interface.",
  "cstar": "The CStaR algorithm (Causal Stability Ranking, Stekhoven, D. J., Moraes, I., Sveinbjörnsson, G., Hennig, L., Maathuis, M. H., & Bühlmann, P. 2012. Causal stability ranking. Bioinformatics, 28(21), 2819-2823) calculates lower bounds on estimated parameters for the causally sufficient case. It first runs a CPDAG algorithm and then for X->Y locally, about Y, finds all possible orientations of the edges in the CPDAG and does an estimation for each of these, and finds their lower bound. In the interface, all nodes that are found in the top bracket more than 50% of the time are marked as into that target node.\nThe procedure is best used, however, as a command line procedure, in py-causal, rpy-causal, or Causal Command. Upon running the algorithm (even in the interface), a directory of result files will be produced as a record, including the dataset used, the possible causes and effects used, all the CPDAGs used and the tables of their IDA effects, and the CStaR output table itself.\n\nParameters.\nAlgorithm. This is the algorithm to use to calculate bootstrapped CPDAGs. Current options are PC Stable, FGES, BOSS, or Restricted BOSS. For large datasets, we recommend Restricted BOSS, which calculates variables with marginal effect on one of the targets and then runs BOSS over this restricted set.\nResults Output Path. A default is “cstar-out”, which will place result-files in a subdirectory of the current directory named path = “cstar-out”.[n], where n is the first index for which no such directory exists. If a directory already exists at the path, then any information available in path directory will be used to generate results in the path-.[n] directory.\nNumber of Sub-samples. CStaR finds CPDAGs over sub-sampled data of size n / 2; this specifies how many sub-samples to use.\nMinimum effect size. This allows a shorter table to be produced. It this is set to a value m > 0, then only records with PI > m will be displayed.\nTarget Names. A list of names of variables (comma or space separated) can be given that are considered possible effects. These will be excluded from the list of possible causes, which will be all other variables in the dataset.\nTop Bracket. The CStaR algorithm tries to find possible causes that regularly sort into the top set of variables by minimum IDA effect. This gives the number q of variables to include in the top bracket, where 1 <= q <= # possible causes.\nParallelized. Yes, if the search should be parallelized, no if not. Default no.\nThe main results of the algorithm is table in the format of Table 1 in Stekhoven et al. here is the beginning of one such table.\nHere, the number of possible causes and the number of possible effects is listed, as well as the top bracket (‘q’) used to generate the table. For each record considered, its cause and effect (e.g., X15 to X20). The percentage of times this cause/effect pair ended up in the top bracket is given as PI, and the minimum IDA effect size for it is given in Effect. The Per Comparison Error Rate (which can only be calculated for PI > 0.5) is given in PCER. See the Stekhoven paper for details.",
  "bdeu-test": "This is a test based on the BDeu score given in Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3), 197-243, used as a test. This gives a score for any two variables conditioned on any list of others which is more positive for distributions which are more strongly dependent. The test for X _||_ Y | Z compares two different models, X conditional on Y, and X conditional on Y and Z; the scores for the two models are subtracted, in that order. If the difference is negative, independence is inferred.",
  "fisher-z-test": "Fisher Z judges independence if the conditional correlation is cannot statistically be distinguished from zero. Primarily for the linear, Gaussian case.",
  "sem-bic-test": "This uses the SEM BIC Score to create a test for the linear, Gaussian case, where we include an additional penalty term, which is commonly used. We call this the penalty discount. So our formulas has BIC = 2L - ck log N,where L is the likelihood, c the penalty discount (usually greater than or equal to 1), and N the sample size. Since the assumption is that the data are distributed as Gaussian, this reduces to BIC = -n log sigma - ck ln N, where sigma is the standard deviation of the linear residual obtained by regressing a child variable onto all of its parents in the model.",
  "poisson-prior-test": "This uses the Poisson Prior Score to create a test for the linear, Gaussian case. See Poisson Prior Score for parameters.",
  "mvplr-test": "Performs a test of conditional independence X _||_ Y | Z1...Zn where all variables are either continuous or discrete. This test is valid for both ordinal and non-ordinal discrete searchVariables.\nAndrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6, 3-18.",
  "gic-bic-tests": "This is a set of generalized information criterion (GIC) scores, used as tests, based on the paper, Kim, Y., Kwon, S., & Choi, H. (2012). Consistent model selection criteria on high dimensions. The Journal 0of Machine Learning Research, 13(1), 1037-1057. One needs to select which lambda to use in place of the usual lambda for the linear, Gaussian BIC score. A penalty discount parameter may also be specified, though this is by default for these scores equal to 1 (since the lambda choice is essentially picking a penalty discount for you). L",
  "mag-sem-bic-test": "This gives a BIC score (used as a test here) for a Mixed Ancestral Graph (MAG). Note that BIC is calculated as 2L - k ln N, so \"higher is better.\"",
  "prob-test": "The Probabilistic Test applies a Bayesian method to derive the posterior probability of an independence constraint R = (X⊥Y|Z) given a dataset D. This is intended for use with datasets with discrete variables. It can be used with constraint-based algorithms (e.g., PC and FCI). Since this test provides a probability for each independence constraint, it can be used stochastically by sampling based on the probabilities of the queried independence constraints to obtain several output graphs. It can also be used deterministically by using a fixed decision threshold on the probabilities of the queried independence constraints to generate a single output graph.",
  "cci-test": "CCI (\"Conditional Correlation Independence\") is a fairly general independence test—not completely general, but general for additive noise models—that is, model in which each variable is equal to a (possibly nonlinear) function of its parents, plus some additive noise, where the noise may be arbitrarily distributed. That is, X = f(parent(X)) + E, where f is any function and E is noise however distributed; the only requirement is that there be the “+” in the formula separating the function from the noise. The noise can’t for instance, be multiplicative, e.g., X = f(parent(X)) x E. The goal of the method is to estimate whether X is independent of Y given variables Z, for some X, Y, and Z. It works by calculating the residual of X given Z and the residual of Y given Z and looking to see whether those two residuals are independent. This test may be used with any constraint-based algorithm (PC, FCI, etc.).",
  "chi-square-test": "This is the usual Chi-Square test for discrete variables; consult an introductory statistics book for details for the unconditional case, where you're just trying, e.g., to determine if X and Y are independent. For the conditional case, the test proceeds as in Fienberg, S. E. (2007). The analysis of cross-classified categorical data, Springer Science & Business Media, by identifying and removing from consideration zero rows or columns in the conditional tables and judging dependence based on the remaining rows and columns.",
  "m-sep-test": "This is the usual test of m-separation, a property of graphs, not distributions. It's not really a test, but it can be used in place of a test of the true graph is known. This is a way to find out, for constraint-based algorithms, or even for some score-based algorithms like FGES, what answer the algorithm would give if all the statistical decisions made are correct. Just draw an edge from the true graph to the algorithm--the m-separation option will appear, and you can then just run the search as usual. Note that D-Separation and M-separation use the same algorithm; we uniformly call the algorithm \"M-Separation\" for clarity. D-Separation is M-Separation applied to DAGs.",
  "disc-bic-test": "This is a BIC score for the discrete case, used as a test. The likelihood is judged by the multinomial tables directly, and this is penalized as is usual for a BIC score. The only surprising thing perhaps is that we use the formula BIC = 2L - k ln N, where L is the likelihood, k the number of parameters, and N the sample size, so \"higher is better.\" In the case of independence, the BIC score will be negative, since the likelihood will be zero, and this will be penalized. The test yields a p-value; we simply use alpha - p as the score, where alpha is the cutoff for rejecting the null hypothesis of independence. This is a number that is positive for dependent cases and negative for independent cases.",
  "g-square-test": "This is completely parallel to the Chi-Square statistic, using a slightly different method for estimating the statistic. The alternative statistic is still distributed as chi-square in the limit. In practice, this statistic is more or less indistinguishable in most cases from Chi-Square. For an explanation, see Spirtes, P., Glymour, C. N., Scheines, R., Heckerman, D., Meek, C., Cooper, G., & Richardson, T. (2000). Causation, prediction, and search. MIT press.",
  "kci-test": "KCI (\"Kernel Conditional Independence\") is a general independence test for model in which X = f(parents(X), eY); here, eY does not need to be additive; it can stand in any functional relationships to the other variables. The variables may even be discrete. The goal of the method is to estimate whether X is independent of Y given Z, completely generally. It uses the kernel trick to estimate this. As a result of using the kernel trick, the method is complex in the direction of sample size, meaning that it may be very slow for large samples. Since it’s slow, individual independence results are always printed to the console so the user knows how far a procedure has gotten. This test may be used with any constraint-based algorithm (PC, FCI, etc.)",
  "cg-lr-test": "Conditional Gaussian Test is a likelihood ratio test based on the conditional Gaussian likelihood function. This is intended for use with datasets where there is a mixture of continuous and discrete variables. It is assumed that the continuous variables are Gaussian conditional on each combination of values for the discrete variables, though it will work fairly well even if that assumption does not hold strictly. This test may be used with any constraint-based algorithm (PC, FCI, etc.). See Andrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6(1), 3-18.",
  "dg-lr-test": "Degenerate Gaussian Likelihood Ratio Test may be used for the case where there is a mixture of discrete and Gaussian variables. Calculates a likelihood ratio based on likelihood that is calculated using a conditional Gaussian assumption. See Andrews, B., Ramsey, J., & Cooper, G. F. (2019). Learning high-dimensional directed acyclic graphs with mixed data-types. Proceedings of machine learning research, 104, 4.",
  "bdeu-score": "This is the BDeu score given in Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3), 197-243. This gives a score for any two variables conditioned on any list of others which is more positive for distributions which are more strongly dependent.",
  "cg-bic-score": "Conditional Gaussian BIC Score may be used for the case where there is a mixture of discrete and Gaussian variables. Calculates a BIC score based on likelihood that is calculated using a conditional Gaussian assumption. See Andrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6(1), 3-18. Note that BIC is calculated as 2L - k ln N, so \"higher is better.\"",
  "dg-bic-score": "Degenerate Gaussian BIC Score may be used for the case where there is a mixture of discrete and Gaussian variables. Calculates a BIC score based on likelihood that is calculated using a conditional Gaussian assumption. See Andrews, B., Ramsey, J., & Cooper, G. F. (2019). Learning high-dimensional directed acyclic graphs with mixed data-types. Proceedings of machine learning research, 104, 4. Note that BIC is calculated as 2L - k ln N, so \"higher is better.\"",
  "m-sep-score": "This uses m-separation to make something that acts as a score if you know the true graph. A score in Tetrad, for FGES, say, is a function that for X and Y conditional on Z, returns a negative number if X _||_ Y | Z and a positive number otherwise. So to get this behavior in no u certain terms, we simply return -1 for independent cases and +1 for dependent cases. Works like a charm. This can be used for FGES to check what the ideal behavior of the algorithm should be. Simply draw an edge from the true graph to the search box, select FGES, and search as usual.",
  "disc-bic-score": "This is a BIC score for the discrete case. The likelihood is judged by the multinomial tables directly, and this is penalized as is usual for a BIC score. The only surprising thing perhaps is that we use the formula BIC = 2L - k ln N, where L is the likelihood, k the number of parameters, and N the sample size, instead of the usual L + k / 2 ln N. So higher BIC scores will correspond to greater dependence. In the case of independence, the BIC score will be negative, since the likelihood will be zero, and this will be penalized.",
  "sem-bic-score": "This is specifically a BIC score for the linear, Gaussian case, where we include an additional penalty term, which is commonly used. We call this the penalty discount. So our formulas has BIC = 2L - ck log N, where L is the likelihood, c the penalty discount (usually greater than or equal to 1), and N the sample size. Since the assumption is that the data are distributed as Gaussian, this reduces to BIC = -n log sigma - ck ln N, where sigma is the standard deviation of the linear residual obtained by regressing a child variable onto all of its parents in the model.",
  "ebic-score": "This is the Extended BIC (EBIC) score of Chen and Chen (Chen, J., & Chen, Z. (2008). Extended Bayesian information criteria for model selection with large model spaces. Biometrika, 95(3), 759-771.). This score is adapted to score-based search in high dimensions. There is one parameter, gamma, which takes a value between 0 and 1; if it's 0, the score is standard BIC. A value of 0.5 or 1 is recommended depending on how many variables there are per sample.",
  "gic-scores": "This is a set of generalized information criterion (GIC) scores based on the paper, Kim, Y., Kwon, S., & Choi, H. (2012). Consistent model selection criteria on high dimensions. The Journal 0of Machine Learning Research, 13(1), 1037-1057. One needs to select which lambda to use in place of the usual lambda for the linear, Gaussian BIC score. A penalty discount parameter may also be specified, though this is by default for these scores equal to 1 (since the lambda choice is essentially picking a penalty discount for you).",
  "mvp-bic-score": "Implements a mixed variable polynomial BIC score. The reference is here: Andrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6, 3-18.\nImplements a mixed variable polynomial BIC score. The reference is here:\nAndrews, B., Ramsey, J., & Cooper, G. F. (2018). Scoring Bayesian networks of mixed variables. International journal of data science and analytics, 6, 3-18.",
  "poisson-prior-score": "This is likelihood score attenuated by the log of the Poisson distribution. It has one parameter, lambda, from the Poisson distribution, which acts as a structure prior.",
  "zsbound-score": "Uses Theorem 1 from Zhang, Y., & Shen, X. (2010). Model selection procedure for high‐dimensional data. Statistical Analysis and Data Mining: The ASA Data Science Journal, 3(5), 350-358, to make a score that controls false positives. The is one parameter, the \"risk bound\", a number between 0 and 1 (a bound on false positive risk probability)."
}